{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXWsgzk2ipV21BAVSlxU04",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yingzibu/JAK_ML/blob/main/EDNE_T2_SE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# paper: Hoffelder et al. (2015). Multivariate equivalence tests for use in pharmaceutical development"
      ],
      "metadata": {
        "id": "J9IRtNS4gRgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy\n",
        "import math\n",
        "from scipy.stats import norm\n",
        "# !pip install latexify-py==0.3.1 --quiet\n",
        "# import latexify"
      ],
      "metadata": {
        "id": "72n2PfMND8r8"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 The EDNE test for equivalence\n",
        "\n",
        "Using as a measure of dissimilarity of the two distributions, the *Euclidean Distance of the Nonstandardized Expected (EDNE)* values as formally being defined by\n",
        "\n",
        "\\begin{equation}\n",
        "\\delta^2_{\\text{EDNE}} := ||\\mu_1 - \\mu_2||^2 = (\\mu_1 - \\mu_2)^\\top (\\mu_1 - \\mu_2) = \\sum_{t=1}^p (\\mu_{1i} - \\mu_{2i})^2\n",
        "\\end{equation}\n",
        "\n",
        "the testing problem to be considered next reads\n",
        "\n",
        "$$H_0 \\delta^2_{\\text{EDNE}} \\geq \\Delta^2_{\\text{EDNE}}$$ versus\n",
        "\n",
        "$$H_A \\delta^2_{\\text{EDNE}} < \\Delta^2_{\\text{EDNE}}$$\n",
        "\n",
        "\n",
        "where the equivalence margin $\\Delta^2_{\\text{EDNE}}$ determines the width of the respective equivalence region. In contrast to the $T^2$-test, the EDNE-test does not rely on the assumption of equal covariance structures, since the target parameter to which the hypotheses relate, involves only the two vectors of population means"
      ],
      "metadata": {
        "id": "AwM3YjdDgfFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$T_{\\text{ENDE}} = \\frac{(\\overline{X} - \\overline{Y})^\\top (\\overline{X} - \\overline{Y}) - \\Delta^2_{\\text{EDNE}}}{\\sqrt{4(\\overline{X} - \\overline{Y})^\\top (\\frac{S_1}{m} + \\frac{S_2}{n})(\\overline{X} - \\overline{Y})}}$$\n",
        "\n",
        "The large-sample test obtained in this way rejects $H_0$ in favor of equivalence if it turns out that there holds $T_EDNE < C=\\phi^{-1} (\\alpha)$, where $\\phi^{-1} (\\alpha)$ denotes the lower $\\alpha$-quantile of the standard normal distribution.\n",
        "\n",
        "Default: $\\Delta^2_{\\text{EDNE}}:= p \\cdot 99$"
      ],
      "metadata": {
        "id": "vW4MdAJJiMM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cal_EDNE(ref_df, test_df, alpha_=0.05, delta_square=None):\n",
        "    if delta_square == None: delta_square = 99 * len(ref_df.columns)\n",
        "\n",
        "    ref_mean = ref_df.mean(axis=0)\n",
        "    test_mean = test_df.mean(axis=0)\n",
        "    diff = ref_mean - test_mean\n",
        "\n",
        "    ref_s = ref_df.cov()\n",
        "    test_s = test_df.cov()\n",
        "\n",
        "    m = len(ref_df); n = len(test_df)\n",
        "    assert m == n\n",
        "\n",
        "    s_pool = ref_s / m + test_s / n\n",
        "    u_ = norm.ppf(alpha_)\n",
        "\n",
        "    t_up = diff.T@ diff - delta_square\n",
        "    t_down = np.sqrt(4 * diff.T @ s_pool @ diff)\n",
        "    t_EDNE = t_up / t_down\n",
        "    if t_EDNE < u_: relation_ = '<'; conc = 'similar'\n",
        "    else: relation_ = '>='; conc = 'dissimilar'\n",
        "    # print('T_EDNE    critical value     conclusion')\n",
        "    # print(f'{t_EDNE:.3f} {relation_} [u_{alpha_}] = {u_:.3f}    {conc}')\n",
        "\n",
        "\n",
        "    temp_ = {}\n",
        "    temp_['EDNE Test'] = [f'| T_EDNE={t_EDNE:.3f}',\n",
        "                         relation_,\n",
        "                         f'[u_{alpha_}] = {u_:.3f}', conc]\n",
        "    temp_df = pd.DataFrame.from_dict(temp_, orient='index',\n",
        "            columns=['Test statistic', 'vs', 'critical value', 'conclusion'])\n",
        "    print(temp_df)\n",
        "    return t_EDNE\n",
        "\n",
        "\n",
        "# cal_EDNE(ref_df, test_df)"
      ],
      "metadata": {
        "id": "yQpj7JwX6Gi7"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The $T^2$-test for equivalence\n",
        "\n",
        "In the $T^2$ test, the Mahalanobis distance $\\delta_{\\text{HOT}}^2 = (\\mu_1 - \\mu_2)^\\top \\Sigma^{-1}(\\mu_1 - \\mu_2)$ is chosen as measure of dissimilarity of the two distributions under comparison. The distributions are considered to be equivalent, if their Mahalanobis distance is sufficiently small\n",
        "\n",
        "More precisely speaking, the hypotheses of the $T^2$-test are\n",
        "\n",
        "$$H_0: \\delta_{\\text{HOT}}^2 \\geq \\Delta_{\\text{HOT}}^2$$\n",
        "\n",
        "versus\n",
        "\n",
        "$$H_A: \\delta_{\\text{HOT}}^2 < \\Delta_{\\text{HOT}}^2$$\n",
        "\n",
        "An exact optimum test for this is based on Hotelling's $T^2$-statistic which is defined by:\n",
        "\n",
        "\\begin{equation}\n",
        "T^2 = \\frac{mn}{m+n}(\\overline{X} - \\overline{Y})^\\top S^{-1} (\\overline{X} - \\overline{Y})\n",
        "\\end{equation}\n",
        "\n",
        "The null hypothesis $H_0$ has to be rejected if $T^2 < C$, where\n",
        "\n",
        "$$C = \\frac{(N-2)p}{N-1-p} F_{p, N-1-p;\\alpha} (\\frac{mn \\Delta^2_{\\text{HOT}}}{N}), N = m + n$$\n",
        "\n",
        "\n",
        "$F_{p, N-1-p;\\alpha} (\\frac{mn \\Delta^2_{\\text{HOT}}}{N})$ denotes the lower quantile of the noncentral F-distribution with p and $N-1-p$ degrees of freedom and noncentrality parameter $\\frac{mn \\Delta^2_{\\text{HOT}}}{N}$, default $\\Delta^2_{\\text{HOT}} = 0.74^2$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WHL9STmg6UGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cal_T_square(ref_df, test_df, alpha_=0.05, delta_square=None):\n",
        "    if delta_square == None: delta_square = 0.74 ** 2\n",
        "\n",
        "    ref_mean = ref_df.mean(axis=0)\n",
        "    test_mean = test_df.mean(axis=0)\n",
        "    diff = ref_mean - test_mean\n",
        "\n",
        "    ref_s = ref_df.cov()\n",
        "    test_s = test_df.cov()\n",
        "\n",
        "    m = len(ref_df); n = len(test_df)\n",
        "    assert m == n\n",
        "    N = m + n\n",
        "    P = len(ref_df.columns)\n",
        "\n",
        "\n",
        "    s = (ref_s + test_s) / 2\n",
        "    s_inv = np.linalg.inv(s)\n",
        "    t_square = m * n / N * diff.T @ s_inv @ diff\n",
        "\n",
        "    # Fcrit = scipy.stats.f.ppf(dfn=P, q=alpha_, dfd=N-P-1) previous\n",
        "    # yet here we have noncentrality parameter, should use ncf function and param nc\n",
        "    nc = delta_square * m * n / N\n",
        "    Fcrit = scipy.stats.ncf.ppf(q=alpha_, dfn=P, dfd=N-P-1, nc=nc, loc=0, scale=1)\n",
        "    adjusted_t_square = (N - 1 - P) / (N - 2) / P * t_square\n",
        "    # C_ = (N-2) * P / (N - 1 - P) * Fcrit\n",
        "    if adjusted_t_square < Fcrit: relation_ = '<'; conc = 'similar'\n",
        "    else: relation_ = '>='; conc = 'dissimilar'\n",
        "\n",
        "    temp_ = {}\n",
        "    temp_['T^2-test'] = [f'| param*T2={adjusted_t_square:.3f}',\n",
        "                         relation_,\n",
        "                         f'[F_{alpha_}] = {Fcrit:.3f}', conc]\n",
        "    temp_df = pd.DataFrame.from_dict(temp_, orient='index',\n",
        "            columns=['Test statistic', 'vs', 'critical value', 'conclusion'])\n",
        "    print(temp_df)\n",
        "    # print(f'test statistic    critical value    conclusion')\n",
        "    # print(f'(N-p-1)/[p(N-2)] T^2 =  {adjusted_t_square:.3f}     {relation_}[F_{alpha_}] = {Fcrit:.3f}   {conc}')\n",
        "    return adjusted_t_square\n",
        "    # try:\n",
        "    #     import latexify\n",
        "    #     @latexify.function(use_math_symbols=True)\n",
        "    #     def C(Fcrit, P, N):\n",
        "    #         # small = p, N-1-p;alpha\n",
        "    #         return (N-2) * P / (N - 1 - P) * Fcrit\n",
        "    #     display(C)\n",
        "\n",
        "    # except: pass\n",
        "    # print(f'T^2      C       critical value    conclusion')\n",
        "    # print(f'{t_square:.3f} {relation_} {C_:.3f} | [F_{alpha_}] = {Fcrit:.3f}   {conc}')\n",
        "    # return t_square\n",
        "\n",
        "# cal_T_square(ref_df, test_df)"
      ],
      "metadata": {
        "id": "IULnpbZu-42B"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The SE-Test for Equivalence\n",
        "\n",
        "In contrat to the EDNE-test, the difference between the population means are now to be assessed as scaled quantities, forming ratios over the variabilities in each vaiable. Choose\n",
        "\n",
        "$$\\delta^2_{\\text{SE}} = \\sum_{i=1}^P\\frac{ (\\mu_{1i}-\\mu_{2i} )^2}{\\frac{1}{2} (\\sigma_{ii}^{(1)}+\\sigma_{ii}^{(2)})}$$\n",
        "\n",
        "as the measure of dissimilarity of the two distributions under comparison, implies that main interest is in the *Standardized Differences of the Expected (SE)* values.\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "T_{\\text{SE}} = \\frac{\\sum_{i=1}^p \\frac{ (\\overline{X}_{i}-\\overline{Y}_{i} )^2}{\\frac{1}{2} (S_{ii}^{(1)}+S_{ii}^{(2)})} - \\Delta^2_{\\text{SE}}   }{\\sqrt{ 16 \\hat{\\delta^*}^\\top  (S_1/m + S_2/n) \\hat{\\delta^*} + 4 \\hat{\\epsilon^*}^\\top (\\hat{\\Pi}_1/m + \\hat{\\Pi}_2/n) \\hat{\\epsilon^*}       }}\n",
        "\\end{equation}\n",
        "\n",
        "where\n",
        "\n",
        "$\\hat{\\delta^*} = \\begin{bmatrix}\n",
        "\\frac{\\overline{X}_1 - \\overline{Y}_1}{S_{11}^{(1)}+S_{11}^{(2)}} \\\\\n",
        "\\vdots \\\\\n",
        "\\frac{\\overline{X}_p - \\overline{Y}_p}{S_{pp}^{(1)}+S_{pp}^{(2)}} \\end{bmatrix}$\n",
        "\n",
        "\n",
        "\n",
        "$\\hat{\\epsilon^*} = \\begin{bmatrix}\n",
        "\\left(\\frac{\\overline{X}_1 - \\overline{Y}_1}{S_{11}^{(1)}+S_{11}^{(2)}} \\right)^2 \\\\\n",
        "\\vdots \\\\\n",
        "\\left(\\frac{\\overline{X}_p - \\overline{Y}_p}{S_{pp}^{(1)}+S_{pp}^{(2)}} \\right)^2 \\end{bmatrix}$\n",
        "\n",
        "\n",
        "\n",
        "$\\hat{\\Pi}_1 = 2 \\begin{bmatrix}\n",
        " (S_{11}^{(1)} )^2 & \\cdots & (S_{1p}^{(1)} )^2 \\\\\n",
        " \\vdots & \\ddots & \\vdots \\\\\n",
        " (S_{p1}^{(1)} )^2 & \\cdots & (S_{pp}^{(1)} )^2\n",
        "\\end{bmatrix}$\n",
        "\n",
        "Similarly for $\\hat{\\Pi}_2$\n",
        "\n",
        "$S_{ij}^{(1)} $ is the entry in the ith row and jth column of empirical covariance matrix $S_1$. Similarly for $S_{11}^{(2)} $ and $S_2$."
      ],
      "metadata": {
        "id": "0rC0n3qFQO_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cal_SE(ref_df, test_df, alpha_=0.05, delta_square=None):\n",
        "    if delta_square == None: delta_square = 0.74 ** 2\n",
        "\n",
        "    ref_mean = ref_df.mean(axis=0)\n",
        "    test_mean = test_df.mean(axis=0)\n",
        "    diff = (ref_mean - test_mean)\n",
        "\n",
        "    ref_s = ref_df.cov()\n",
        "    test_s = test_df.cov()\n",
        "\n",
        "    m = len(ref_df); n = len(test_df)\n",
        "    assert m == n\n",
        "    N = m + n\n",
        "    P = len(ref_df.columns)\n",
        "\n",
        "    s_list = [ref_s.iloc[i,i] + test_s.iloc[i,i] for i in range(P)]\n",
        "    hat_delta = diff.div(pd.Series(s_list, index=ref_df.columns), axis=0)\n",
        "    hat_epsilon = np.multiply(hat_delta, hat_delta)\n",
        "\n",
        "    t_up = 2 * hat_delta.T @ diff - delta_square\n",
        "    s_pool = (ref_s/m + test_s/n)\n",
        "\n",
        "    t_down = 16 * hat_delta.T @ s_pool @ hat_delta\n",
        "    hat_pi_1 = 2 * np.multiply(ref_s, ref_s)\n",
        "    hat_pi_2 = 2 * np.multiply(test_s, test_s)\n",
        "    t_down += 4 * hat_epsilon.T @ (hat_pi_1 / m + hat_pi_2 / n) @ hat_epsilon\n",
        "    t_se = t_up / np.sqrt(t_down)\n",
        "\n",
        "    u_ = norm.ppf(alpha_)\n",
        "    if t_se < u_: relation_ = '<'; conc = 'similar'\n",
        "    else: relation_ = '>='; conc = 'dissimilar'\n",
        "\n",
        "    temp_ = {}\n",
        "    temp_['SE Test'] = [f'| T_SE={t_se:.3f}',\n",
        "                         relation_,\n",
        "                         f'[u_{alpha_}] = {u_:.3f}', conc]\n",
        "    temp_df = pd.DataFrame.from_dict(temp_, orient='index',\n",
        "            columns=['Test statistic', 'vs', 'critical value', 'conclusion'])\n",
        "    print(temp_df)\n",
        "    return t_se\n",
        "\n",
        "\n",
        "\n",
        "# cal_SE(ref_df, test_df), cal_SE(ref_df[[15,20]], test_df[[15,20]])"
      ],
      "metadata": {
        "id": "TaZ8SKRGUmjy"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table 1"
      ],
      "metadata": {
        "id": "0vJVcNF8czuo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbQyzc-BgNYK",
        "outputId": "9de31584-e508-41d7-e1b4-9d91ff2fbe8a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(    15  20  25\n",
              " 0   49  86  98\n",
              " 1   15  59  96\n",
              " 2   56  84  96\n",
              " 3   57  87  99\n",
              " 4    6  58  90\n",
              " 5   62  90  97\n",
              " 6   23  71  97\n",
              " 7   11  64  92\n",
              " 8    9  61  88\n",
              " 9   42  81  96\n",
              " 10  57  86  98\n",
              " 11   4  48  82,\n",
              "     15  20   25\n",
              " 0   31  79   94\n",
              " 1   13  64   97\n",
              " 2   55  81   96\n",
              " 3   50  72   86\n",
              " 4   37  91   99\n",
              " 5   47  88   96\n",
              " 6   39  87   97\n",
              " 7    3  33   80\n",
              " 8   37  77   94\n",
              " 9   34  74   92\n",
              " 10   4  59   91\n",
              " 11  11  60  100)"
            ]
          },
          "metadata": {},
          "execution_count": 224
        }
      ],
      "source": [
        "## Table 1\n",
        "\n",
        "\n",
        "ref_df = pd.DataFrame()\n",
        "ref_df[15] = pd.DataFrame([49,15,56,57,6,62,23,11,9,42,57,4])\n",
        "ref_df[20] = pd.DataFrame([86,59,84,87,58,90,71,64,61,81,86,48])\n",
        "ref_df[25] = pd.DataFrame([98,96,96,99,90,97,97,92,88,96,98,82])\n",
        "\n",
        "test_df = pd.DataFrame()\n",
        "test_df[15] = pd.DataFrame([31,13,55,50,37,47,39,3,37,34,4,11])\n",
        "test_df[20] = pd.DataFrame([79,64,81,72,91,88,87,33,77,74,59,60])\n",
        "test_df[25] = pd.DataFrame([94,97,96,86,99,96,97,80,94,92,91,100])\n",
        "ref_df, test_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cal_f2(ref_df, test_df, bc=False, vc=False, ver=False): # implement bias corrected\n",
        "    # input is df, could do bc vc\n",
        "    if type(ref_df) == pd.DataFrame: ref_mean = list(ref_df.mean(axis=0))\n",
        "    # input is a list of mean, cannot do vc, bias corrected\n",
        "    elif type(ref_df) == list:       ref_mean = ref_df; bc, vc=False, False\n",
        "    else: print('unrecognized for ref data, type={type(ref_df)}'); return\n",
        "\n",
        "    if type(test_df) == pd.DataFrame: test_mean = list(test_df.mean(axis=0))\n",
        "    elif type(test_df) == list:       test_mean = test_df; bc, vc=False, False\n",
        "    else: print('unrecognized for ref data, type={type(test_df)}'); return\n",
        "\n",
        "    P = len(ref_mean) # number of time points\n",
        "    assert len(ref_mean) == len(test_mean)\n",
        "    sum_diff_square = 0\n",
        "    for i, j in zip(ref_mean, test_mean):\n",
        "        sum_diff_square += (i-j) ** 2\n",
        "\n",
        "    sum_variance = 0\n",
        "    if vc or bc: # will apply var or bias corrected only inf ref and test are dataframes\n",
        "        try: assert len(ref_df) == len(test_df)\n",
        "        except: print('Different unit number between ref and test'); return\n",
        "\n",
        "        ref_S = [i**2 for i in np.std(ref_df, ddof=1).tolist()]\n",
        "        test_S= [i**2 for i in np.std(test_df, ddof=1).tolist()]\n",
        "        if bc and vc == False:\n",
        "            sum_variance =  np.sum(ref_S) + np.sum(test_S)\n",
        "        if vc: # apply variance-corrected f2\n",
        "            bc = True\n",
        "            sum_variance = 0\n",
        "            for rs, ts in zip(ref_S, test_S):\n",
        "                sum_s = rs + ts\n",
        "                w_t = 0.5 + ts / sum_s\n",
        "                w_r = 0.5 + rs / sum_s\n",
        "                sum_variance += w_t * ts + w_r * rs\n",
        "        n = len(ref_df)   # number of units\n",
        "        sum_variance /= n\n",
        "\n",
        "    if sum_variance > sum_diff_square: # definitely applied bc or vc\n",
        "        if vc: param_name = 'vc'; vc = False\n",
        "        else: param_name = 'bc' ; bc = False\n",
        "\n",
        "        print(f'var    >   sum(|t-r|^2), cannot apply {param_name}')\n",
        "        print(f'{sum_variance:.3f} > {sum_diff_square:.3f}')\n",
        "        sum_variance = 0\n",
        "    # else: # 2 conditions, sum_variance=0, sum_variance \\in (0, sum_diff_square)\n",
        "\n",
        "        # reset bc = False, vc = Fal\n",
        "\n",
        "    D = sum_diff_square - sum_variance\n",
        "\n",
        "    f2 = 100 - 25 * np.log10(1+D/P)\n",
        "    print(f'F2 value R & T: {f2:.3f} | bc: {bc} | vc: {vc}')\n",
        "    if ver: return f2, sum_variance, sum_diff_square\n",
        "    else: return f2\n",
        "\n",
        "\n",
        "def cal_MSD(ref_df, test_df, tolerance_list=[10,11,13,15]):\n",
        "    try:\n",
        "        assert list(test_df.columns) == list(ref_df.columns)\n",
        "    except:\n",
        "        print(f'time diff: test{list(test_df.columns)} ref {list(ref_df.columns)}')\n",
        "        return\n",
        "    time_points = list(test_df.columns)\n",
        "\n",
        "    P = len(time_points)\n",
        "    n = len(ref_df)\n",
        "\n",
        "    try: assert n == len(test_df)\n",
        "    except:\n",
        "        print(f'ref units {n} are different from test units {len(test_df)}')\n",
        "        print('Check data before cal MSD'); return\n",
        "\n",
        "    S1 = ref_df.cov()\n",
        "    S2 = test_df.cov()\n",
        "    S_pooled = (S1 + S2) / 2\n",
        "    ref_mean = list(ref_df.mean(axis=0))\n",
        "    test_mean = list(test_df.mean(axis=0))\n",
        "    x2_x1 = [i-j for i, j in zip(test_mean, ref_mean)]\n",
        "    a = np.array(x2_x1).reshape(len(time_points), 1)\n",
        "    K = n**2/(2*n)* (2*n - P - 1) / ((2*n - 2) * P)\n",
        "    Fcrit = scipy.stats.f.ppf(q=1-.1, dfn=P, dfd=2*n-P-1)\n",
        "    spinv = np.linalg.inv(S_pooled.loc[time_points, time_points])\n",
        "    D_M = np.sqrt(a.T @ spinv @ a)[0][0]\n",
        "    print('Mahalanobis distance (T & R):', D_M)\n",
        "\n",
        "    bound1 = a @ (1 + np.sqrt(Fcrit/(K * a.T @ spinv @ a)))\n",
        "    bound2 = a @ (1 - np.sqrt(Fcrit/(K * a.T @ spinv @ a)))\n",
        "    # 90% CI of Mahalanobis distance:\n",
        "    DM_1 = np.sqrt(bound1.T @ spinv @ bound1)[0][0]\n",
        "    DM_2 = np.sqrt(bound2.T @ spinv @ bound2)[0][0]\n",
        "    DM_upper = max(DM_1, DM_2)\n",
        "    DM_lower = min(DM_1, DM_2)\n",
        "\n",
        "    print('lower bound of DM:', DM_lower)\n",
        "    print('upper bound of DM:', DM_upper)\n",
        "\n",
        "\n",
        "    print('DM_upper | tolerance limit | conclusion')\n",
        "    for tolerance in tolerance_list:\n",
        "\n",
        "        D_g = np.array([tolerance] * len(time_points)).reshape(len(time_points), 1)\n",
        "        RD = np.sqrt(D_g.T @ spinv @ D_g)[0][0]\n",
        "\n",
        "        if DM_upper <= RD:\n",
        "            print(f'{DM_upper:.3f} \\t <=  {RD:.3f}[{tolerance}%]     Similar')\n",
        "        else:\n",
        "            print(f'{DM_upper:.3f} \\t >   {RD:.3f} [{tolerance}%]    Dissimilar')"
      ],
      "metadata": {
        "id": "bQJVH9eUlsQx"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class dis_data:\n",
        "    def __init__(self, df, time_points=None, col_name='Time', ref_test=['Ref', 'Test']):\n",
        "        if type(df) == pd.DataFrame: # input is the whole df directly\n",
        "            data_df = df.copy()\n",
        "            ref_df = data_df[data_df[col_name]==ref_test[0]]\n",
        "            test_df = data_df[data_df[col_name]==ref_test[1]]\n",
        "        elif type(df) == list: # [ref_df, test_df]\n",
        "            ref_df = df[0]; test_df = df[1]\n",
        "        if time_points == None:\n",
        "            print('did not specify time points, will automatically select')\n",
        "            time_points = []\n",
        "            time_points_float = []\n",
        "            for i in ref_df.columns:\n",
        "                try:\n",
        "                    time_num = float(i)\n",
        "                    time_points.append(i)\n",
        "                    time_points_float.append(time_num)\n",
        "                except: pass\n",
        "        print('selected time points: ', time_points)\n",
        "        self.time_points = time_points\n",
        "        self.time_points_float = time_points_float\n",
        "\n",
        "\n",
        "        self.ref_df = ref_df[self.time_points].reset_index(drop=True)\n",
        "        self.test_df = test_df[self.time_points].reset_index(drop=True)\n",
        "        self.ref_mean = list(self.ref_df.mean(axis=0))\n",
        "        self.test_mean = list(self.test_df.mean(axis=0))\n",
        "\n",
        "        # when they calculate standard deviation, they use ddof=1,\n",
        "        # divided by N-1 instead of N\n",
        "        # calculate standard deviation for ref_df and test_df\n",
        "        self.ref_sd = list(self.ref_df.std(ddof=1))\n",
        "        self.test_sd = list(self.test_df.std(ddof=1))\n",
        "\n",
        "        # calculate cv for ref and test data, cv = sd / mean at each time point\n",
        "        self.ref_cv = [i/j for i, j in zip(self.ref_sd, self.ref_mean)]\n",
        "        self.test_cv = [i/j for i, j in zip(self.test_sd, self.test_mean)]\n",
        "\n",
        "        dict_here = {}\n",
        "        dict_here['time_point'] = self.time_points\n",
        "        dict_here['ref_mean']  = self.ref_mean\n",
        "        dict_here['test_mean'] = self.test_mean\n",
        "        dict_here['ref_cv']  = self.ref_cv\n",
        "        dict_here['test_cv'] = self.test_cv\n",
        "        self.stats = pd.DataFrame.from_dict(dict_here)\n",
        "        self.idx_list = [i for i in range(len(self.time_points))]\n",
        "        self.idx_list_rule_85 = []\n",
        "\n",
        "\n",
        "    def view_data(self):\n",
        "        print('*'*50)\n",
        "        print(' reference data extracted:\\n', self.ref_df)\n",
        "        print('\\n test data extracted:\\n', self.test_df)\n",
        "        print('\\n ---> Mean and CV calculated:\\n', self.stats)\n",
        "        print('*'*50)\n",
        "\n",
        "    def plot_data(self, xlabel='time in minutes', ylabel='% dissolved',\n",
        "                  title='Mean distribution of the test and reference'):\n",
        "        x_axis = self.time_points_float\n",
        "        test_axis = self.test_mean\n",
        "        ref_axis = self.ref_mean\n",
        "        plt.plot(x_axis, test_axis, '.-', label='test batch')\n",
        "        plt.plot(x_axis, ref_axis, '.-', label='ref batch')\n",
        "        plt.xlabel(xlabel)\n",
        "        plt.ylabel(ylabel)\n",
        "        plt.title(title)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "    def apply_85_rule(self, rule_85=True):\n",
        "        idx_list = []\n",
        "        if rule_85: # will evaluate 85% rule,\n",
        "            if len(self.idx_list_rule_85) == 0: # select from scratch\n",
        "                print('---> Include 1 time point of average dissolution >85% only')\n",
        "                first_85_appended = False\n",
        "                for idx, (i, r, t) in enumerate(\n",
        "                    zip(self.time_points, self.ref_mean, self.test_mean)):\n",
        "                    if r > 85 and t > 85:\n",
        "                        print(f't = {i}, mean of ref {r:.3f} & test {t:.3f} > 85%',\n",
        "                              end=\", \")\n",
        "                        if first_85_appended == False:\n",
        "                            idx_list.append(idx) # will include one time point exceeding 85%,\n",
        "                            print(f'preserve time {i}')\n",
        "                            first_85_appended = True\n",
        "                        else: print(f'delete time {i}')\n",
        "                    else: idx_list.append(idx)\n",
        "                self.idx_list_rule_85 = idx_list # update rule_85 index list\n",
        "                print('85% rule index list updated')\n",
        "\n",
        "            # else: #  len(self.idx_list_rule_85) > 0:\n",
        "            # self.idx_list_rule_85 is not empty, do not need to cal again\n",
        "            return self.idx_list_rule_85\n",
        "\n",
        "        else: #  rule_85 == False\n",
        "            # print('Did not apply 85% rule, will calculate on all data')\n",
        "            # idx_list = [i for i in range(len(self.time_points))]\n",
        "            return self.idx_list\n",
        "\n",
        "    def data_apply_85(self, rule_85=True, ver_idx=False):\n",
        "        idx_list = self.apply_85_rule(rule_85=rule_85)\n",
        "        time_points = list(np.take(self.time_points, idx_list))\n",
        "        ref_df = self.ref_df[time_points]\n",
        "        test_df = self.test_df[time_points]\n",
        "        if ver_idx: return time_points, ref_df, test_df, idx_list\n",
        "        return time_points, ref_df, test_df\n",
        "\n",
        "\n",
        "    def cal_f2(self, rule_85=True, ver=True):\n",
        "        \"\"\"\n",
        "        Use of the f2 metric is allowed provided that following constraints are met:\n",
        "         -    >= 3 time-points\n",
        "         -    85% rule (<= 1 time point should be included with average dissolution >85%)\n",
        "         -    CV <= 20% for 1st point, <=10% for other points\n",
        "        \"\"\"\n",
        "        print('\\n', '*'*40, 'F2 calculation', '*'*40)\n",
        "        print('Apply 85% rule? : ', rule_85)\n",
        "        time_here, ref_df, test_df, idx_list = self.data_apply_85(\n",
        "                                                    rule_85=rule_85,\n",
        "                                                    ver_idx=True)\n",
        "\n",
        "        print('\\nEvaluate whether f2 is suitable :')\n",
        "\n",
        "        if len(time_here) < 3:\n",
        "            print('* WARNING: time points < 3, f2 may not be suitable')\n",
        "        else:\n",
        "            print(f'* Satisfy criteria for f2: {len(time_here)} time points, larger than 3')\n",
        "\n",
        "        # evaluate CV<0.2 for first time point, CV < 0.1 for other time points rule\n",
        "        test_cv_here = list(np.take(self.test_cv, idx_list))\n",
        "        ref_cv_here  = list(np.take(self.ref_cv,  idx_list))\n",
        "\n",
        "        cv_cond = True\n",
        "        for idx, (i, t, r) in enumerate(zip(time_here, test_cv_here, ref_cv_here)):\n",
        "            if idx == 0: # check whether CV <20%\n",
        "                if t > 0.2 or r > 0.2:\n",
        "                    print('CV at first time point exceeds 20%, f2 may not be suitable')\n",
        "                    cv_cond = False\n",
        "            else:\n",
        "                if t > 0.1 or r > 0.1:\n",
        "                    print(f'At time {i}, CV exceeds 10%, f2 may not be suitable')\n",
        "                    cv_cond = False\n",
        "        if cv_cond: print('* Satisfy CV criteria for f2 calculation')\n",
        "        print()\n",
        "        return cal_f2(ref_df, test_df, ver=ver)\n",
        "\n",
        "\n",
        "    def cal_MSD(self, rule_85=False, tolerance_list=[10,11,13,15]):\n",
        "        print('\\n', '*'*40, 'MSD calculation', '*'*40)\n",
        "        print('Apply 85% rule? : ', rule_85)\n",
        "        time_points, ref_df, test_df = self.data_apply_85(rule_85=rule_85)\n",
        "\n",
        "        cal_MSD(ref_df, test_df, tolerance_list=tolerance_list)\n",
        "\n",
        "    def cal_EDNE(self, rule_85=False, alpha_=0.05, delta_square=None):\n",
        "        print('\\n', '*'*40, 'EDNE calculation', '*'*40)\n",
        "        print('Apply 85% rule? : ', rule_85)\n",
        "        time_points, ref_df, test_df = self.data_apply_85(rule_85=rule_85)\n",
        "\n",
        "        return cal_EDNE(ref_df, test_df,\n",
        "                        alpha_=alpha_, delta_square=delta_square)\n",
        "\n",
        "    def cal_T_square(self, rule_85=False, alpha_=0.05, delta_square=None):\n",
        "        print('\\n', '*'*40, 'T square calculation', '*'*40)\n",
        "        print('Apply 85% rule? : ', rule_85)\n",
        "        time_points, ref_df, test_df = self.data_apply_85(rule_85=rule_85)\n",
        "\n",
        "        return cal_T_square(ref_df, test_df,\n",
        "                        alpha_=alpha_, delta_square=delta_square)\n",
        "\n",
        "    def cal_SE(self, rule_85=False, alpha_=0.05, delta_square=None):\n",
        "        print('\\n', '*'*40, 'SE calculation', '*'*40)\n",
        "        print('Apply 85% rule? : ', rule_85)\n",
        "        time_points, ref_df, test_df = self.data_apply_85(rule_85=rule_85)\n",
        "\n",
        "        return cal_SE(ref_df, test_df,\n",
        "                        alpha_=alpha_, delta_square=delta_square)"
      ],
      "metadata": {
        "id": "EQooBgvOkC_d"
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Table 4, p = 2\n",
        "new_data = dis_data([ref_df[[15,20]], test_df[[15,20]]])\n",
        "# new_data.cal_f2()\n",
        "# new_data.cal_MSD(tolerance_list=[10])\n",
        "new_data.cal_T_square()\n",
        "new_data.cal_EDNE()\n",
        "new_data.cal_()\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5FSo0owLZYP",
        "outputId": "6c8f5739-689a-4739-81ff-94019860870c"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "did not specify time points, will automatically select\n",
            "selected time points:  [15, 20]\n",
            "\n",
            " **************************************** T square calculation ****************************************\n",
            "Apply 85% rule? :  False\n",
            "            Test statistic vs    critical value    conc.\n",
            "T^2-test  | param*T2=0.073  <  [F_0.05] = 0.241  similar\n",
            "\n",
            " **************************************** EDNE calculation ****************************************\n",
            "Apply 85% rule? :  False\n",
            "            Test statistic vs     critical value conclusion\n",
            "EDNE Test  | T_EDNE=-3.692  <  [u_0.05] = -1.645    similar\n",
            "\n",
            " **************************************** SE calculation ****************************************\n",
            "Apply 85% rule? :  False\n",
            "        Test statistic vs     critical value conclusion\n",
            "SE Test  | T_SE=-3.817  <  [u_0.05] = -1.645    similar\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Table 5, p = 3\n",
        "\n",
        "new_data = dis_data([ref_df, test_df])\n",
        "# new_data.cal_f2()\n",
        "# new_data.cal_MSD(tolerance_list=[10])\n",
        "new_data.cal_T_square()\n",
        "new_data.cal_EDNE()\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTOSRuvx5EEL",
        "outputId": "9b020652-da60-439e-cd01-09a16fae1a62"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "did not specify time points, will automatically select\n",
            "selected time points:  [15, 20, 25]\n",
            "\n",
            " **************************************** T square calculation ****************************************\n",
            "Apply 85% rule? :  False\n",
            "            Test statistic vs    critical value    conc.\n",
            "T^2-test  | param*T2=0.103  <  [F_0.05] = 0.319  similar\n",
            "\n",
            " **************************************** EDNE calculation ****************************************\n",
            "Apply 85% rule? :  False\n",
            "            Test statistic vs     critical value conclusion\n",
            "EDNE Test  | T_EDNE=-5.439  <  [u_0.05] = -1.645    similar\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQH-PzpAyO1g",
        "outputId": "4b9eacc3-cbc0-4a82-a760-853d76746946"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-5.438830513315034"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpha_ = 0.05\n",
        "\n",
        "u_95"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjyg5Herxczo",
        "outputId": "8db8ff7f-c713-4eba-b13d-f1b6faad02a2"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.6448536269514722"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diff.T @ diff"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGzF6NFOxgzP",
        "outputId": "ff16e6a4-babb-4612-ea73-7edf33336882"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.284722222222251"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum = 0\n",
        "for i in diff:\n",
        "    sum += i**2\n",
        "sum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bdv31B_3xioe",
        "outputId": "a269b1c4-7233-4743-c7ea-91c46a341e24"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.284722222222251"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cal_f2(ref_df, test_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhfwvRbsxkRx",
        "outputId": "18019199-5c23-4532-a8cf-94d60b37e885"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F2 calculated for R & T: 86.623 | bc: False | vc: False\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86.62321720734256"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cal_EDNE(ref_df[[15,20]], test_df[[15,20]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZ7R8HEw0NV3",
        "outputId": "5126f1ff-8fbf-4acd-8c0f-97556a5a2158"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T_EDNE    critical value     conclusion\n",
            "-3.692 < [u_0.05] = -1.645    similar\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-3.6920136929552263"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sWHDViD83cC-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "9q1sOSSa_izC"
      ],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMxmzeFoaIArgMsQYvdDHh5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yingzibu/JAK_ML/blob/main/examples/expeirments/finetune/ADMET_10_30_ST_scale_cetain_metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQoOGJ4n4Qhs",
        "outputId": "3b5edb00-f815-4408-a2bf-ec17fd5f4edc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print('cuda: ', torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jvRBq2b4jwb",
        "outputId": "3779ef14-b6a4-4428-e83f-9fd80fb6e00d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu118\n",
            "cuda:  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install rdkit --quiet\n",
        "! pip install PyTDC --quiet\n",
        "! pip install mycolorpy --quiet\n",
        "! pip install selfies  --quiet\n",
        "! pip install pubchempy --quiet\n",
        "! pip install dgllife --quiet\n",
        "! pip install molvs --quiet\n",
        "! pip install dgl -f https://data.dgl.ai/wheels/cu118/repo.html --quiet\n",
        "! pip install dglgo -f https://data.dgl.ai/wheels-test/repo.html --quiet\n",
        "\n",
        "! pip install DeepPurpose --quiet\n",
        "! pip install git+https://github.com/bp-kelley/descriptastorus --quiet\n",
        "! pip install pandas-flavor --quiet"
      ],
      "metadata": {
        "id": "7mMn2Ig84oe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Code"
      ],
      "metadata": {
        "id": "9q1sOSSa_izC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import walk\n",
        "import os\n",
        "file_types = ['bin', 'pth']\n",
        "\n",
        "# clean certain type of file in path\n",
        "def clean_files(path='/content/drive/MyDrive/ADMET/', file_types = ['pth']):\n",
        "    files = next(walk(path), (None, None, []))[2]\n",
        "    for file in files:\n",
        "        if isinstance(file, str):\n",
        "            file_type = file.split('.')[-1]\n",
        "            if file_type in file_types:\n",
        "                os.remove(file); print(f'{file} removed from {path}')"
      ],
      "metadata": {
        "id": "ZAoenGDh4s4X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/ADMET"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNYbARcw4wBg",
        "outputId": "535aa545-c603-416e-bf47-f8c89f440794"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ADMET\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test scripts.func_utils.py\n",
        "\n",
        "from scripts.func_utils import make_path, convert_with_qed_sa, get_min, \\\n",
        "                                plot_loss, plot_performance\n",
        "\n",
        "from scripts.eval_utils import *\n",
        "from scripts.preprocess_mols import *\n",
        "from scripts.model_architecture import *\n",
        "from scripts.dataset import *\n",
        "from scripts.train import *\n",
        "from tdc.single_pred import ADME\n",
        "from tdc.single_pred import Tox\n",
        "from tdc.utils import retrieve_label_name_list\n",
        "import pandas as pd\n",
        "\n",
        "label_list = retrieve_label_name_list('herg_central')\n",
        "\n",
        "def collect_data_10_27(names:list, clean_mol_=False, verbose=False):\n",
        "    if isinstance(names, str): names = [names]\n",
        "    name_adme = ['Caco2_Wang', 'Lipophilicity_AstraZeneca',\n",
        "                 'HydrationFreeEnergy_FreeSolv',\n",
        "                 'Solubility_AqSolDB'] # regression task\n",
        "    name_adme+= ['CYP2C19_Veith', 'CYP2D6_Veith', 'CYP3A4_Veith',\n",
        "                'CYP1A2_Veith', 'CYP2C9_Veith'] + \\\n",
        "                ['BBB_Martins', 'Bioavailability_Ma', 'Pgp_Broccatelli',\n",
        "                 'HIA_Hou','PAMPA_NCATS'] # classify\n",
        "    print('collect data for: ', names)\n",
        "    label_list = retrieve_label_name_list('herg_central')\n",
        "    for i, name in enumerate(names):\n",
        "        if verbose: print('*'*15, name, '*'*15)\n",
        "        if name in label_list: data = Tox(name='herg_central', label_name=name)\n",
        "        elif name in name_adme: data = ADME(name=name)\n",
        "        else:\n",
        "            try: data = Tox(name=name)\n",
        "            except: print('cannot read data!'); return\n",
        "            if verbose: data.label_distribution()\n",
        "            # data.label_distribution()\n",
        "        split = data.get_split()\n",
        "        train, valid, test = split['train'], split['valid'], split['test']\n",
        "        if clean_mol_:\n",
        "            train, valid, test = clean_mol(train), clean_mol(valid), clean_mol(test)\n",
        "\n",
        "        train = rename_cols(train[['Drug', 'Y']], name)\n",
        "        valid = rename_cols(valid[['Drug', 'Y']], name)\n",
        "        test  = rename_cols(test[['Drug', 'Y']],  name)\n",
        "\n",
        "        if i == 0: trains, valids, tests = train.copy(), valid.copy(), test.copy()\n",
        "        else:\n",
        "            trains = trains.merge(train, how='outer')\n",
        "            valids = valids.merge(valid, how='outer')\n",
        "            tests = tests.merge(test, how='outer')\n",
        "\n",
        "    return trains, valids, tests\n",
        "# trains, valids, tests = collect_data_10_27(names_reg[0])\n",
        "\n",
        "from scripts.get_vocab import *\n",
        "\n",
        "def get_multi_loader(trains, valids, tests, config):\n",
        "    names = config['prop_names']\n",
        "    vocab = None if 'vocab' not in config else config['vocab']\n",
        "    batch_size = config['batch_size']\n",
        "    model_type = config['model_type']\n",
        "\n",
        "    print('---> loader for', names)\n",
        "    params_ = {'batch_size': batch_size, 'shuffle': True,\n",
        "               'drop_last': False, 'num_workers': 0}\n",
        "    param_t = {'batch_size': batch_size, 'shuffle': False,\n",
        "               'drop_last': False, 'num_workers': 0}\n",
        "\n",
        "    # NEED TO CHANGE HERE TO INCLUDE SELFIES\n",
        "    if model_type == 'RNN'and vocab == None:\n",
        "        df = pd.concat([trains, valids, tests], ignore_index=True, axis=0)\n",
        "        vocab = get_vocab(df)\n",
        "    train_loader = get_loader(trains, names, params_, model_type, vocab)\n",
        "    valid_loader = get_loader(valids, names, params_, model_type, vocab)\n",
        "    test_loader  = get_loader(tests,  names, param_t, model_type, vocab)\n",
        "    return train_loader, valid_loader, test_loader, vocab\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NbG922x4zLX",
        "outputId": "39f64d5b-ab34-48a1-ad1d-c2febea72179"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VOCAB_TYPE: smiles, could change from ['char', 'smiles', 'selfies'] at get_vocab.py\n",
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import yaml\n",
        "def yml_report(yml_path, recalculate=False, ver=False):\n",
        "    \"\"\"\n",
        "    given yml path or yml data, return the test set performance\n",
        "    param\n",
        "        yml_path        : str,  the path of yml file\n",
        "        recalculate     : bool, if true, will calculate from scratch\n",
        "        ver             : bool, if true, will print detailed configs\n",
        "    return perfm_dict   : dict, contain performance and test loss\n",
        "    \"\"\"\n",
        "    if isinstance(yml_path, str): # the path string was input\n",
        "        with open(yml_path, 'r') as f: data = yaml.safe_load(f)\n",
        "    elif isinstance(yml_path, dict):   data = yml_path # data was input\n",
        "\n",
        "    config = data['config']\n",
        "    model_type = config['model_type']\n",
        "    task_names = config['prop_names']\n",
        "    perfm_dict =  data['performance']\n",
        "\n",
        "    if len(perfm_dict) != 0 and recalculate == False:\n",
        "        # during training, has evaluted test set, no need to calculate again\n",
        "        # However for regression, the pred vs true value for test is not saved\n",
        "        # if need the regression pred vs true value graph, need recalculate\n",
        "        if ver: # print model config, and the training saved info\n",
        "            print('#'*68); print('#'*30, 'CONFIG', '#'*30); print('#'*68)\n",
        "            for i, j in config.items(): print(i, ':', j)\n",
        "            print('#'*68)\n",
        "            print('Model parameters: ', data['params_num'])\n",
        "            times_list = data['times_list']\n",
        "            print(f'Train time: {np.mean(times_list):.3f}'\n",
        "                  f'+/-{np.std(times_list):.3f} ms')\n",
        "            print(f\"best epoch: {data['best_epoch']}, \",\n",
        "                  f\"min loss: {data['min_loss']:.4f}\")\n",
        "            plot_loss(data['train_dict'], data['valid_dict'], name='valid',\n",
        "                      title_name= f'loss during training {model_type}')\n",
        "\n",
        "    else: # recalculate from scratch using test data\n",
        "        vocab = None if 'vocab' not in config else config['vocab']\n",
        "        trains, valids, tests = collect_data_10_27(task_names)\n",
        "        if config['scale_dict'] != None: # scale is done\n",
        "            trains, valids, tests, dict_scale = scale(trains, valids, tests)\n",
        "            assert config['scale_dict'] == dict_scale\n",
        "        batch_size = config['batch_size']\n",
        "        param_t = {'batch_size': batch_size, 'shuffle': False,\n",
        "                'drop_last': False, 'num_workers': 0}\n",
        "        test_loader = get_loader(tests, task_names, param_t, model_type, vocab)\n",
        "        models = PRED(**config); models.load_status(data)\n",
        "        outputs = models.eval(test_loader, ver=ver)\n",
        "        perfm_dict = outputs[0]\n",
        "    return perfm_dict\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "#                0      1        2       3        4     5      6      7      8\n",
        "cls_metrics = ['acc', 'w_acc', 'prec', 'recall', 'sp', 'f1', 'auc', 'mcc', 'ap']\n",
        "reg_metrics = ['mae', 'mse', 'rmse', 'r2']\n",
        "d = {'reg': [0,   2,    3], 'cls': [0,   5,  6]}\n",
        "#            mae, rmse, r2          acc, f1, auc\n",
        "\n",
        "\n",
        "# evaluate performance list, if directly from yml_file saved performance (dict)\n",
        "# need to [yml_file_data['performance']] to convert dict into list\n",
        "def eval_perf_list(perfs:list, name:list,\n",
        "                   metrics_dict=d, # could be {} # eval all\n",
        "                   reg_metrics_all=reg_metrics,\n",
        "                   cls_metrics_all=cls_metrics):\n",
        "    \"\"\"\n",
        "    The same model type for multiple times, performance saved in list perfs\n",
        "    Aim: evaluate performance of name, calculate mean and std for multiple run\n",
        "    : param metrics_dict: dict, if None, print all metrics\n",
        "                          example: {'reg': [0, 2, 3], 'cls': [0, 5, 6]}\n",
        "    \"\"\"\n",
        "    if len(metrics_dict) == 0:  # will print all metrics\n",
        "        metrics_dict['reg'] = [i for i in range(len(reg_metrics_all))]\n",
        "        metrics_dict['cls'] = [i for i in range(len(cls_metrics_all))]\n",
        "    if isinstance(name, str): name = [name]\n",
        "    if isinstance(perfs, dict): perfs = [perfs]\n",
        "    repeat_time = len(perfs) # the same model was run for # repeat_time times\n",
        "\n",
        "    if repeat_time > 1: # multiple run, find the lowest loss\n",
        "        loss_list = [p['loss'] for p in perfs]\n",
        "        best_model_idx = np.argmin(loss_list) # has the lowest loss\n",
        "        best_perf = perfs[best_model_idx]\n",
        "        print('repeated num #', repeat_time, end=\" \")\n",
        "        print(f'idx {best_model_idx} has the lowest loss from {loss_list}')\n",
        "    else: best_perf = None\n",
        "    for n in name:\n",
        "        IS_R = names_dict[n]\n",
        "        # print(n, '\\tRegression?', IS_R);\n",
        "        if IS_R:\n",
        "              idxs = metrics_dict['reg']; ms=[reg_metrics_all[i] for i in idxs]\n",
        "        else: idxs = metrics_dict['cls']; ms=[cls_metrics_all[i] for i in idxs]\n",
        "\n",
        "        results = {}\n",
        "        for idx, i in enumerate(perfs): # access idx_th evaluation in perfs\n",
        "            r = i[n]; results[idx] = r  # collect the evaluation for name n\n",
        "\n",
        "        means, stds = [], []\n",
        "\n",
        "        for idx_v in range(len(r)):\n",
        "            cur_values = []\n",
        "            for idx in range(repeat_time):\n",
        "                cur_v = results[idx][idx_v]; cur_values.append(cur_v)\n",
        "            mean_here, std_here = np.mean(cur_values), np.std(cur_values)\n",
        "            means.append(mean_here); stds.append(std_here)\n",
        "            # print(f'{ms[idx_v]}\\t: {mean_here:.3f} +/- {std_here:.3f}')\n",
        "\n",
        "        print('*'*20, n, '*'*20,  end=' \\n\\t')\n",
        "        for k in ms: print('\\t', k, end = ' \\t ')\n",
        "        print()\n",
        "\n",
        "        for idx_final, (i, j) in enumerate(zip(means, stds)):\n",
        "            if idx_final == 0:    print(end='\\t| ')\n",
        "            if idx_final in idxs: print(f'{i:.3f} +/- {j:.3f}', end=' | ')\n",
        "        if best_perf != None:\n",
        "            print(f'\\n idx {best_model_idx}: ', end='| ')\n",
        "            for i in idxs:\n",
        "                print(f'{best_perf[n][i]:.3f} +/- {0:.3f}', end=' | ')\n",
        "        print('\\n')\n",
        "\n",
        "        # break\n"
      ],
      "metadata": {
        "id": "uIvL_r0w5SRa"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Constant and configs"
      ],
      "metadata": {
        "id": "G0SyupsH_ekP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### CONSTANTS ###\n",
        "names_reg = ['Caco2_Wang', 'Lipophilicity_AstraZeneca',\n",
        "         'HydrationFreeEnergy_FreeSolv', 'Solubility_AqSolDB', 'LD50_Zhu'] # regression task\n",
        "names_cls = ['CYP2C19_Veith', 'CYP2D6_Veith', 'CYP3A4_Veith',\n",
        "             'CYP1A2_Veith', 'CYP2C9_Veith'] + \\\n",
        "            ['BBB_Martins', 'Bioavailability_Ma',\n",
        "             'Pgp_Broccatelli', 'HIA_Hou','PAMPA_NCATS'] + \\\n",
        "            ['hERG_Karim', 'AMES']\n",
        "\n",
        "names_dict = {}\n",
        "for name in names_reg + names_cls:\n",
        "    if name in names_reg:   names_dict[name] = True  # regression task\n",
        "    elif name in names_cls: names_dict[name] = False # classification task\n",
        "names_all = list(names_dict.keys())\n",
        "\n",
        "model_types = ['MLP', 'AttentiveFP', 'GIN', 'RNN']\n",
        "\n",
        "#                0      1        2       3        4     5      6      7      8\n",
        "cls_metrics = ['acc', 'w_acc', 'prec', 'recall', 'sp', 'f1', 'auc', 'mcc', 'ap']\n",
        "reg_metrics = ['mae', 'mse', 'rmse', 'r2']\n"
      ],
      "metadata": {
        "id": "9d1Fn3LT43ct"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_dim = 256\n",
        "hid_dims = [128, 64, 16]\n",
        "dropout = 0.5\n",
        "lr = 3e-4\n",
        "wd = 1e-5\n",
        "MAX_EPOCH = 1000\n",
        "patience = 30           # stop if loss no decrease after epochs # patience\n",
        "verbose_frequency = 100 # print evaluation every # verbose_frequency epoch\n",
        "batch_size = 128\n",
        "\n",
        "# special for AttentiveFP\n",
        "graph_feat_size = 300\n",
        "n_layers = 5\n",
        "num_timesteps = 1   # times of updating the graph representations with GRU\n",
        "\n",
        "# special for GIN: pretrain model types for selection:\n",
        "pre_models_GIN = ['gin_supervised_contextpred', 'gin_supervised_infomax',\n",
        "                     'gin_supervised_edgepred', 'gin_supervised_masking']\n",
        "pre_model_num = 0    # choose from pre_models for GIN\n",
        "\n",
        "\n",
        "# if VOCAB_TYPE == 'smiles':\n",
        "import yaml\n",
        "def load_vocab(VOCAB_TYPE):\n",
        "    try:\n",
        "        with open(f'vocab/{VOCAB_TYPE}.yml', 'r') as f: data = yaml.safe_load(f)\n",
        "        vocab = data['vocab']; assert VOCAB_TYPE == data['vocab_type']\n",
        "    except: vocab = None\n",
        "    return vocab\n",
        "\n",
        "# special for RNN:\n",
        "VOCAB = load_vocab(VOCAB_TYPE)\n",
        "Bid = True\n",
        "GRU_num_layers = 3\n",
        "GRU_dim = 256\n",
        "\n",
        "\n",
        "\n",
        "scale_dict = None\n",
        "\n",
        "def get_config(model_type, names,\n",
        "               pre_model_num=pre_model_num, scale_dict=scale_dict):\n",
        "    \"\"\"\n",
        "    Get config to initialize model\n",
        "        param model_type: str, ['MLP', 'AttentiveFP', 'GIN', 'RNN']\n",
        "        param names: list, task names\n",
        "        param scale_dict: dict,\n",
        "            if the task is regression, could scale label values\n",
        "                            {name: [value_min, value_max], ...}\n",
        "        param pre_model_num: int, [0, 1, 2, 3]\n",
        "            if model_type is 'GIN', 4 types of pretrained models to choose from\n",
        "    Returns config that could be used as PRED(**config)\n",
        "    \"\"\"\n",
        "    pre_models_GIN = ['gin_supervised_contextpred', 'gin_supervised_infomax',\n",
        "                         'gin_supervised_edgepred', 'gin_supervised_masking']\n",
        "\n",
        "    # print(scale_dict)\n",
        "    if isinstance(names, str): names = [names]\n",
        "    IS_R = [names_dict[name] for name in names]\n",
        "    config_MLP = {'model_type': 'MLP',\n",
        "            'in_dim': 167,\n",
        "            'hid_dims': hid_dims,\n",
        "            'out_dim': len(names),\n",
        "            'prop_names': names,\n",
        "            'dropout': dropout,\n",
        "            'IS_R': IS_R,\n",
        "            'batch_size': batch_size,\n",
        "            'lr': lr,\n",
        "            'wd': wd,\n",
        "            'patience': patience,\n",
        "            'verbose_freq': verbose_frequency,\n",
        "            'model_path': f'ckpt_MLP.pt',\n",
        "            'scale_dict': scale_dict}\n",
        "\n",
        "    config_ATF = {'model_type': 'AttentiveFP',\n",
        "            'graph_feat_size': graph_feat_size,\n",
        "            'num_timesteps': num_timesteps,\n",
        "            'n_layers': n_layers,\n",
        "            'out_dim': len(names),\n",
        "            'prop_names': names,\n",
        "            'dropout': dropout,\n",
        "            'IS_R': IS_R,\n",
        "            'batch_size': batch_size,\n",
        "            'lr': lr,\n",
        "            'wd': wd,\n",
        "            'patience': patience,\n",
        "            'verbose_freq': verbose_frequency,\n",
        "            'model_path': 'ckpt_AT.pt',\n",
        "            'scale_dict': scale_dict}\n",
        "\n",
        "    config_GIN = {'model_type': 'GIN',\n",
        "            'pretrain_model': pre_models_GIN[pre_model_num],\n",
        "            'in_dim': in_dim,\n",
        "            'hid_dims': hid_dims,\n",
        "            'out_dim': len(names),\n",
        "            'prop_names': names,\n",
        "            'dropout': dropout,\n",
        "            'batch_size': batch_size,\n",
        "            'IS_R': IS_R,\n",
        "            'lr': lr,\n",
        "            'wd': wd,\n",
        "            'patience': patience,\n",
        "            'verbose_freq': verbose_frequency,\n",
        "            'model_path': f'ckpt_GIN_{pre_models_GIN[pre_model_num]}.pt',\n",
        "            'scale_dict': scale_dict}\n",
        "\n",
        "    config_RNN = {'model_type': 'RNN',\n",
        "              'vocab': VOCAB,\n",
        "              'vocab_type': VOCAB_TYPE,\n",
        "              'Bidirect': Bid,\n",
        "              'num_layers': GRU_num_layers,\n",
        "              'GRU_dim': GRU_dim,\n",
        "              'out_dim': len(names),\n",
        "              'prop_names': names,\n",
        "              'dropout': dropout,\n",
        "              'IS_R': IS_R,\n",
        "              'device': 'cuda',\n",
        "              'batch_size': batch_size,\n",
        "              'lr': lr,\n",
        "              'wd': wd,\n",
        "              'patience': patience,\n",
        "              'verbose_freq': verbose_frequency,\n",
        "              'model_path': f'ckpt_RNN_{VOCAB_TYPE}.pt',\n",
        "              'scale_dict': scale_dict}\n",
        "\n",
        "    if model_type == 'MLP':           con_MO = config_MLP\n",
        "    elif model_type == 'AttentiveFP': con_MO = config_ATF\n",
        "    elif model_type == 'GIN':         con_MO = config_GIN\n",
        "    elif model_type == 'RNN':         con_MO = config_RNN\n",
        "    else: print('Error !{MLP, AttentiveFP, GIN, RNN}'); return None\n",
        "    con_MO['config_path'] = con_MO['model_path'].split('.')[0] + '.yml'\n",
        "    # different weight of task, initial weight the same\n",
        "    con_MO['weight_loss'] = [float(1.0/len(names))] * len(names)\n",
        "    con_MO['MAX_EPOCH'] = MAX_EPOCH\n",
        "    return con_MO\n",
        "\n"
      ],
      "metadata": {
        "id": "_21eb4UC_VZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yml_name = 'Lipophilicity_AstraZeneca_scale/AttentiveFP_ST_1.yml'\n",
        "p = yml_report(yml_name, ver=False)\n",
        "print('Evaluate all metrics, just set metrics_dict as {}')\n",
        "eval_perf_list(p, 'Lipophilicity_AstraZeneca', metrics_dict={})\n",
        "\n",
        "print(f'Evaluate selected metrics {d}')\n",
        "eval_perf_list(p, 'Lipophilicity_AstraZeneca', metrics_dict=d)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXi-gurF5ZqS",
        "outputId": "d1c13a29-c984-41e6-82c9-fb08ff710bba"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluate all metrics, just set metrics_dict as {}\n",
            "******************** Lipophilicity_AstraZeneca ******************** \n",
            "\t\t mae \t \t mse \t \t rmse \t \t r2 \t \n",
            "\t| 0.353 +/- 0.000 | 0.239 +/- 0.000 | 0.489 +/- 0.000 | 0.834 +/- 0.000 | \n",
            "\n",
            "Evaluate selected metrics {'reg': [0, 2, 3], 'cls': [0, 5, 6]}\n",
            "******************** Lipophilicity_AstraZeneca ******************** \n",
            "\t\t mae \t \t rmse \t \t r2 \t \n",
            "\t| 0.353 +/- 0.000 | 0.489 +/- 0.000 | 0.834 +/- 0.000 | \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yml_name = '/content/drive/MyDrive/ADMET/M5/MLP_MT_1.yml'\n",
        "p = yml_report(yml_name, ver=False)\n",
        "name = ['CYP2C19_Veith', 'CYP2D6_Veith', 'CYP3A4_Veith',\n",
        "                         'CYP1A2_Veith', 'CYP2C9_Veith']\n",
        "\n",
        "eval_perf_list(p, name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1f6HDmV5u-F",
        "outputId": "85dc6390-50df-4f49-e397-1e4d0967525b"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "******************** CYP2C19_Veith ******************** \n",
            "\t\t acc \t \t f1 \t \t auc \t \n",
            "\t| 0.776 +/- 0.000 | 0.762 +/- 0.000 | 0.847 +/- 0.000 | \n",
            "\n",
            "******************** CYP2D6_Veith ******************** \n",
            "\t\t acc \t \t f1 \t \t auc \t \n",
            "\t| 0.843 +/- 0.000 | 0.315 +/- 0.000 | 0.815 +/- 0.000 | \n",
            "\n",
            "******************** CYP3A4_Veith ******************** \n",
            "\t\t acc \t \t f1 \t \t auc \t \n",
            "\t| 0.726 +/- 0.000 | 0.667 +/- 0.000 | 0.816 +/- 0.000 | \n",
            "\n",
            "******************** CYP1A2_Veith ******************** \n",
            "\t\t acc \t \t f1 \t \t auc \t \n",
            "\t| 0.816 +/- 0.000 | 0.808 +/- 0.000 | 0.900 +/- 0.000 | \n",
            "\n",
            "******************** CYP2C9_Veith ******************** \n",
            "\t\t acc \t \t f1 \t \t auc \t \n",
            "\t| 0.795 +/- 0.000 | 0.666 +/- 0.000 | 0.862 +/- 0.000 | \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test single train scaling effects"
      ],
      "metadata": {
        "id": "9llWpDaaAPd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_multi_run(model_type, name, folder_list, repeat_time=3, run_type=None):\n",
        "    if isinstance(name, str): name = [name]\n",
        "    if run_type == None: run_type = 'ST' if len(name) == 1 else 'MT'\n",
        "    if isinstance(folder_list, str): folder_list = [folder_list]\n",
        "\n",
        "    for folder_ in folder_list:\n",
        "        print('checking', folder_)\n",
        "        try:\n",
        "            perform_list = []\n",
        "            for i in range(repeat_time):\n",
        "                yml_name = f'{folder_}/{model_type}_{run_type}_{i}.yml'\n",
        "                try:\n",
        "                    p = yml_report(yml_name, ver=False); perform_list.append(p)\n",
        "                except:\n",
        "                    print('cannot read', yml_name); break\n",
        "            if len(perform_list) != 0: eval_perf_list(perform_list, name)\n",
        "        except: pass\n",
        "    print()"
      ],
      "metadata": {
        "id": "DFYiiZeIAOvB"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'Lipophilicity_AstraZeneca'\n",
        "folder_list = [f'{name}_scale']\n",
        "for model_type in model_types:\n",
        "    print(model_type)\n",
        "    eval_multi_run(model_type, name, folder_list, run_type='ST')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FY4y21EAOzO",
        "outputId": "68d5d85f-47c6-4024-ad1a-23bf886bdc00"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP\n",
            "checking Lipophilicity_AstraZeneca_scale\n",
            "repeated num # 3\n",
            "******************** Lipophilicity_AstraZeneca ******************** \n",
            "\t\t mae \t \t rmse \t \t r2 \t \n",
            "\t| 0.663 +/- 0.011 | 0.864 +/- 0.007 | 0.495 +/- 0.008 | \n",
            " idx 0: | 0.654 +/- 0.000 | 0.855 +/- 0.000 | 0.506 +/- 0.000 | \n",
            "\n",
            "\n",
            "AttentiveFP\n",
            "checking Lipophilicity_AstraZeneca_scale\n",
            "repeated num # 3\n",
            "******************** Lipophilicity_AstraZeneca ******************** \n",
            "\t\t mae \t \t rmse \t \t r2 \t \n",
            "\t| 0.365 +/- 0.009 | 0.503 +/- 0.011 | 0.824 +/- 0.007 | \n",
            " idx 1: | 0.353 +/- 0.000 | 0.489 +/- 0.000 | 0.834 +/- 0.000 | \n",
            "\n",
            "\n",
            "GIN\n",
            "checking Lipophilicity_AstraZeneca_scale\n",
            "repeated num # 3\n",
            "******************** Lipophilicity_AstraZeneca ******************** \n",
            "\t\t mae \t \t rmse \t \t r2 \t \n",
            "\t| 0.522 +/- 0.028 | 0.683 +/- 0.030 | 0.684 +/- 0.028 | \n",
            " idx 0: | 0.498 +/- 0.000 | 0.660 +/- 0.000 | 0.706 +/- 0.000 | \n",
            "\n",
            "\n",
            "RNN\n",
            "checking Lipophilicity_AstraZeneca_scale\n",
            "repeated num # 3\n",
            "******************** Lipophilicity_AstraZeneca ******************** \n",
            "\t\t mae \t \t rmse \t \t r2 \t \n",
            "\t| 0.624 +/- 0.007 | 0.836 +/- 0.014 | 0.527 +/- 0.016 | \n",
            " idx 2: | 0.614 +/- 0.000 | 0.817 +/- 0.000 | 0.549 +/- 0.000 | \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Test single task scaling effect"
      ],
      "metadata": {
        "id": "1qE-6UZ8GL7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names_reg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owBDcn0p61mq",
        "outputId": "2ff112e8-b70e-4298-94c3-f0dd29e0634c"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Caco2_Wang',\n",
              " 'Lipophilicity_AstraZeneca',\n",
              " 'HydrationFreeEnergy_FreeSolv',\n",
              " 'Solubility_AqSolDB',\n",
              " 'LD50_Zhu']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repeat_time = 3\n",
        "for name in names_reg[:1]:\n",
        "    run_type = 'ST'\n",
        "    trn, val, tst = collect_data_10_27([name])\n",
        "    for scale_task in [False, True]:\n",
        "        if scale_task == False: folder_name = f'{name}_no_scale'\n",
        "        else: folder_name = f'{name}_scale'\n",
        "        make_path(folder_name)\n",
        "        print('\\n\\n\\n\\n\\ninfo will be saved at', folder_name)\n",
        "        trn, val, tst, dict_scale = scale(trn, val, tst, scale_task=scale_task)\n",
        "\n",
        "        for model_type in model_types:\n",
        "            print(model_type)\n",
        "            config = get_config(model_type, [name])\n",
        "            config['scale_dict'] = dict_scale\n",
        "            trn_l, val_l, tst_l, vocab = get_multi_loader(trn, val, tst, config)\n",
        "            if vocab != None and config['vocab'] == None:\n",
        "                config['vocab'] = vocab # update config vocab info\n",
        "                print('RNN, no vocab, use data update | vocab len:', len(vocab))\n",
        "\n",
        "            perfs = [] # a list to store the performance outputted from the model\n",
        "            for i in range(repeat_time):\n",
        "                print(f'\\nRun # {i} for {model_type} {run_type}', end='\\t')\n",
        "                save_dir = f'{folder_name}/{model_type}_{run_type}_{i}'\n",
        "                config['model_path']  = save_dir + '.pt'\n",
        "                config['config_path'] = save_dir + '.yml'\n",
        "\n",
        "                try: # try open yml file, if file exists, and no need train\n",
        "                    with open(config['config_path'], 'r') as f:\n",
        "                        data = yaml.safe_load(f)\n",
        "                    if data != None:\n",
        "                        p = yml_report(data); print(' pre data loaded \\n')\n",
        "                except:  # model was not trained yet, train the model\n",
        "                    models = PRED(**config)\n",
        "                    p = models.train(trn_l, val_l, tst_l)\n",
        "                perfs.append(p)\n",
        "            eval_perf_list(perfs, name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OR8fVGc751qx",
        "outputId": "bacfdc63-6b7f-4661-d0be-d2bf17b0c901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:15 [Train] Loss: 5.718 | [Valid] Loss: 2.487\tSAVE MODEL: loss: 2.574 -> 2.487 | runtime: 2.499 ms\n",
            "Epoch:16 [Train] Loss: 4.879 | [Valid] Loss: 1.979\tSAVE MODEL: loss: 2.487 -> 1.979 | runtime: 2.438 ms\n",
            "Epoch:17 [Train] Loss: 5.021 | [Valid] Loss: 1.671\tSAVE MODEL: loss: 1.979 -> 1.671 | runtime: 2.435 ms\n",
            "Epoch:18 [Train] Loss: 4.772 | [Valid] Loss: 1.473\tSAVE MODEL: loss: 1.671 -> 1.473 | runtime: 2.551 ms\n",
            "Epoch:19 [Train] Loss: 4.487 | [Valid] Loss: 1.407\tSAVE MODEL: loss: 1.473 -> 1.407 | runtime: 2.483 ms\n",
            "Epoch:20 [Train] Loss: 3.966 | [Valid] Loss: 1.203\tSAVE MODEL: loss: 1.407 -> 1.203 | runtime: 2.565 ms\n",
            "Epoch:21 [Train] Loss: 4.344 | [Valid] Loss: 0.924\tSAVE MODEL: loss: 1.203 -> 0.924 | runtime: 2.492 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repeat_time = 3\n",
        "for name in names_reg[3:]:\n",
        "    run_type = 'ST'\n",
        "    trn, val, tst = collect_data_10_27([name])\n",
        "    for scale_task in [False, True]:\n",
        "        if scale_task == False: folder_name = f'{name}_no_scale'\n",
        "        else: folder_name = f'{name}_scale'\n",
        "        make_path(folder_name)\n",
        "        print('\\n\\n\\n\\n\\ninfo will be saved at', folder_name)\n",
        "        trn, val, tst, dict_scale = scale(trn, val, tst, scale_task=scale_task)\n",
        "\n",
        "        for model_type in model_types:\n",
        "            print(model_type)\n",
        "            config = get_config(model_type, [name])\n",
        "            config['scale_dict'] = dict_scale\n",
        "            trn_l, val_l, tst_l, vocab = get_multi_loader(trn, val, tst, config)\n",
        "            if vocab != None and config['vocab'] == None:\n",
        "                config['vocab'] = vocab # update config vocab info\n",
        "                print('RNN, no vocab, use data update | vocab len:', len(vocab))\n",
        "\n",
        "            perfs = [] # a list to store the performance outputted from the model\n",
        "            for i in range(repeat_time):\n",
        "                print(f'\\nRun # {i} for {model_type} {run_type}', end='\\t')\n",
        "                save_dir = f'{folder_name}/{model_type}_{run_type}_{i}'\n",
        "                config['model_path']  = save_dir + '.pt'\n",
        "                config['config_path'] = save_dir + '.yml'\n",
        "\n",
        "                try: # try open yml file, if file exists, and no need train\n",
        "                    with open(config['config_path'], 'r') as f:\n",
        "                        data = yaml.safe_load(f)\n",
        "                    if data != None:\n",
        "                        p = yml_report(data); print(' pre data loaded \\n')\n",
        "                except:  # model was not trained yet, train the model\n",
        "                    models = PRED(**config)\n",
        "                    p = models.train(trn_l, val_l, tst_l)\n",
        "                perfs.append(p)\n",
        "            eval_perf_list(perfs, name)"
      ],
      "metadata": {
        "id": "rrnI0LGa7GH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_perf_list([p], name[:2], metrics_dict=d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Wus0c3v7ZrT",
        "outputId": "9d272975-ed88-469d-ce61-de217aa5a083"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "repeated num # 1\n",
            "******************** CYP2C19_Veith ******************** \n",
            "\t\t acc \t \t f1 \t \t auc \t \n",
            "\t| 0.776 +/- 0.000 | 0.762 +/- 0.000 | 0.847 +/- 0.000 | \n",
            "\n",
            "\n",
            "repeated num # 1\n",
            "******************** CYP2D6_Veith ******************** \n",
            "\t\t acc \t \t f1 \t \t auc \t \n",
            "\t| 0.843 +/- 0.000 | 0.315 +/- 0.000 | 0.815 +/- 0.000 | \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "32QYRaOH8Wyg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}